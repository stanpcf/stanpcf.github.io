<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[tensorflow数据与模型的结合]]></title>
    <url>%2F2019%2Ftensorflow%E6%95%B0%E6%8D%AE%E4%B8%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%BB%93%E5%90%88%2F</url>
    <content type="text"><![CDATA[引言tensorflow主流的编程姿势有三种: 使用low level api 使用keras 使用estimator 一个比较好的机器学习程序是data与model的分离。data作为数据流流入model从而训练model。tensorflow的这三种编程姿势也是按照这个趋势来走的。特别是在这几天出来的tensorflow2.0版本里面将tf.Session编程tf.function，更是将这一思想表现的淋漓尽致。而在estimator、keras这些高阶api里面，这一思想更不用说。 我们都知道做实验都需要将材料放入反应堆里面，不同的材料从不同的入口流进反应堆。同样在tensorflow里面也是这样的，一条数据(比如说seq2seq模型，一条数据经过处理达到可以直接灌入模型前通常会被处理成三部分, ids, text_len, masks)灌入模型，数据的不同部分需要从不同的入口进入模型。 所以模型入口和数据的不同部分(特征)是一一对应的。 在tensorflow里面，所有的编程姿势都离不开上面这句话。本篇博客将简要介绍一下上面的这个思想，从模型入口、数据的不同部分、他们如何一一对应这三要素讲解如上三种tensorflow编程姿势。 low level api这部分是tensorflow早期(1.x时代的)一种编程模式 其模型入口使用tf.placeholder来定义，需要指定shape, type。 其数据的不同部分编程者心里有数就行， 每一部分都使用list来表示 而他们如何一一对应使用的是tf.Session.run这个方法的feed_dict参数来实现的，这个参数接受一个dict, dict的key是tf.placeholder的实例，这个实例定义了从这个入口进入的数据的type及类型。如下是tensorflow1.1.0版本的mnist官方实现的feed_dict， 1234567891011121314151617181920212223def fill_feed_dict(data_set, images_pl, labels_pl): """Fills the feed_dict for training the given step. A feed_dict takes the form of: feed_dict = &#123; &lt;placeholder&gt;: &lt;tensor of values to be passed for placeholder&gt;, .... &#125; Args: data_set: The set of images and labels, from input_data.read_data_sets() images_pl: The images placeholder, from placeholder_inputs(). labels_pl: The labels placeholder, from placeholder_inputs(). Returns: feed_dict: The feed dictionary mapping from placeholders to values. """ # Create the feed_dict for the placeholders filled with the next # `batch size` examples. images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size, FLAGS.fake_data) feed_dict = &#123; images_pl: images_feed, labels_pl: labels_feed, &#125; return feed_dict 我们可以看到其就是使用placeholder同数据的不同部分的一一对应来实现模型的训练的 keras其模型入口使用keras.Input来定义，需要指定shape, type。 其数据的不同部分编程者心里有数就行， 每一部分都使用list或者generator来表示 而他们如何一一对应使用的是keras.Model.fit这个方法的x、y参数来实现的，这两个参数接受一个dict或者list, 或者单个的数据array。如果为dict的话，那么在模型入口这个keras.Input必须指定name参数，这个name参数就是这个dict的key, value为从这个入口进去的数据。如果为list的话，那么在model=Model(inputs, outputs)里面，inputs必须为list，且同x必须长度相等，同一个index位置代表着入口和一一对应的数据如果为单个array的话，那么自然就一一对应了 如下是keras的编程的一个example 12345678910111213141516171819data = np.random.random((1000, 32))labels = np.random.random((1000, 10))inputs = tf.keras.Input(shape=(32,)) # Returns a placeholder tensor# A layer instance is callable on a tensor, and returns a tensor.x = layers.Dense(64, activation='relu')(inputs)x = layers.Dense(64, activation='relu')(x)predictions = layers.Dense(10, activation='softmax')(x)model = tf.keras.Model(inputs=inputs, outputs=predictions)# The compile step specifies the training configuration.model.compile(optimizer=tf.train.RMSPropOptimizer(0.001), loss='categorical_crossentropy', metrics=['accuracy'])# Trains for 5 epochsmodel.fit(data, labels, batch_size=32, epochs=5) estimator在estimator里面需要实例化一个Estimator来编程。实例化的时候需要传入model_fn来定义模型。而在Estimator.train里面传入input_fn来给模型传入数据。那么如何实现上面的三要素的呢 其模型入口使用model_fn来实现，model_fn签名为model_fn(features, labels, mode, parmas, config)，features, labels应该为单输入或者是dict。单输入则为tf.Tensor，dict的的key为一个字符串，value为tf.Tensor。 其数据的不同部分需要input_fn来实现，这个函数需要返回features，labels。如果模型单输入就无所谓了，直接返回。如果是多输入则需要features和labels各自返回dict，dict的key为字符串，value为数据的不同部分。 而他们如何一一对应。如果是单输入自然不存在一一对应的问题。如果多输入的话，从上面我们可以看到，model_fn的features同input_fn的返回的features都为dict, 他们的key即是一一对应的标识符。input_fn的features数据的各部分根据他们的key寻找model_fn的features里面对应的key的value(即tf.Tensor)。根据这个key实现两个value的一一对应。 这个地方需要注意的是，在写model_fn的时候需要在config参数里面定义feature_columns。 在1.x里面通过tf.feature_columns.input_layer来实现model_fn的features与feature_columns绑定。feature_columns定义了各个key(这里model_fn与input_fn的key相同就不用解释了)的类型与shape。 在2.0时代tf.feature_columns.input_layer这个函数不存在了，取而代之是InputLayer这个函数。主要原因是tensorflow的趋势是一切皆keras编程，所以这个函数只接受feature_columns，然后模仿的是keras的Layer行为，在call函数里面去传入features，所以行为演变成了keras的行为，即Input与model=Model(inputs, outputs)定义了模型的入口。 在custom_estimator.py里面有如何使用estimator编程，注意这是1.x版本的tensorflow。]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow_hub]]></title>
    <url>%2F2019%2Ftensorflow-hub%2F</url>
    <content type="text"><![CDATA[tensorflow hub是用来加载预训练模型的，比如说bert模型什么的。本篇博客讲述如何训练导出一个tensorflow hub 所拥有的模型。然后如何加载这么一个模型 如下是训练导出一个tensorflow hub模块所接受的模型12345678910111213141516171819202122232425262728293031323334353637383940414243444546import tensorflow as tfimport tensorflow_hub as hub'''2 - 建立一个网络结构，并基于该网络结构建立一个Module '''def half_plus_two(): '''该函数主要是创建一个简单的模型，其网络结构就是y = a*x + b ''' # 创建两个变量，a和b，如网络的权重和偏置 a = tf.get_variable('a', shape=[]) b = tf.get_variable('b', shape=[]) # 创建一个占位变量，为后面graph的输入提供准备 x = tf.placeholder(tf.float32) # 创建一个完整的graph y = a*x + b # 通过hub的add_signature，建立hub需要的网络 # 这个函数决定了如何去加载这个模型。 hub.add_signature(name='info', inputs=&#123;'x': x&#125;, outputs=&#123;'y': y&#125;)def export_module(path): '''该函数用于调用创建api进行module创建，然后进行网络的权重赋值，最后通过session进行运行权重初始化，并最后输出该module''' # 通过hub的create_module_spec，接收函数建立一个Module spec = hub.create_module_spec(half_plus_two) # 防止串graph，将当期的操作放入同一个graph中 with tf.Graph().as_default(): # 通过hub的Module读取一个模块，该模块可以是url链接，表示从tensorflow hub去拉取， # 或者接收上述创建好的module module = hub.Module(spec) # 这里演示如何将权重值赋予到graph中的变量，如从checkpoint中进行变量恢复等 init_a = tf.assign(module.variable_map['a'], 0.5) init_b = tf.assign(module.variable_map['b'], 2.0) init_vars = tf.group([init_a, init_b]) with tf.Session() as sess: # 运行初始化，为了将其中变量的值设置为赋予的值 sess.run(init_vars) # 将模型导出到指定路径 module.export(path, sess)def main(argv): export_module('hub_dict')if __name__ == '__main__': tf.app.run(main) 加载上面代码所创建的模型的代码： 123456789101112131415161718192021222324252627282930313233343536import osimport subprocessimport tensorflow as tfimport tensorflow_hub as hubclass HalfPlusTwoTests(tf.test.TestCase): def testExportTool(self): # 指定module的文件夹位置，这里是export module_path = os.path.join('.', 'hub_dict') with tf.Graph().as_default(): # 读取当前存在的一个module m = hub.Module(module_path) # 如直接采用y=f(x) 一样进行调用， # 这个地方调用hub.Module.__call__函数，这个函数比较重要的参数是inputs, signature, as_dict # 这个地方如何使用取决于hub.add_signature如何构建网络。 # add_signature的name控制hub.Module.__call__的signature参数。如果name为None的话,则name默认设置为default，hub.Module.__call__的signature参数不用传参，如果传参必须为default。如果name不是默认参数的话，那么signature必须同name是同一个字符串。 # add_signature的inputs控制hub.Module.__call__的inputs, 如果add_signature的inputs为多输入的话(dict)，则hub.Module.__call__的inputs必须为dict, 且key必须相同, value同add_signature的inputs的value所定义的placeholder相同 # add_signature的outputs控制hub.Module.__call__的as_dict, 如果outputs为多个输出的话(必须为dict形式)，则as_dict必须为True。call函数返回一个dict, 每一项的key对应于outputs里面的每一项 output = m(&#123;'x': [10, 3, 4]&#125;, signature='info', as_dict=True) print(output) with tf.Session() as sess: # 惯例进行全局变量初始化 sess.run(tf.initializers.global_variables()) # 观察生成的值是否与预定义值一致，即prediction是否与label一致 self.assertAllEqual(sess.run(output['y']), [7, 3.5, 4])if __name__ == '__main__': tf.test.main() 官方讲解tensorflowHub的文档 https://www.tensorflow.org/hub]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.estimator]]></title>
    <url>%2F2019%2Ftf-estimator%2F</url>
    <content type="text"><![CDATA[estimatorestimator同keras是tensorflow的高级API。在tensorflow1.13以上，estimator已经作为一个单独的package从tensorflow分离出来了。estimator抽象了tensorflow底层的api, 同keras一样，他分离了model和data, 不同于keras这个不得不认养的儿子，estimator作为tensorflow的亲儿子，天生具有分布式的基因，更容易在生产环境里面使用 tensorflow官方文档提供了比较详细的estimator程序的构建过程:https://www.tensorflow.org/guide#estimators tensorflow model提供了estimator构建的mnist程序:https://github.com/tensorflow/models/blob/master/official/mnist/mnist.py estimator模型由model_fn决定:官方文档: 其中features, labels是必需的。model, params, config参数是可选的如下是estiamtor定义的一个模型: 12345678910111213141516171819202122232425262728293031323334353637383940def my_model(features, labels, mode, params): """DNN with three hidden layers and learning_rate=0.1.""" # Create three fully connected layers. net = tf.feature_column.input_layer(features, params['feature_columns']) for units in params['hidden_units']: net = tf.layers.dense(net, units=units, activation=tf.nn.relu) # Compute logits (1 per class). logits = tf.layers.dense(net, params['n_classes'], activation=None) # Compute predictions. predicted_classes = tf.argmax(logits, 1) if mode == tf.estimator.ModeKeys.PREDICT: predictions = &#123; 'class_ids': predicted_classes[:, tf.newaxis], 'probabilities': tf.nn.softmax(logits), 'logits': logits, &#125; return tf.estimator.EstimatorSpec(mode, predictions=predictions) # Compute loss. loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits) # Compute evaluation metrics. accuracy = tf.metrics.accuracy(labels=labels, predictions=predicted_classes, name='acc_op') metrics = &#123;'accuracy': accuracy&#125; tf.summary.scalar('accuracy', accuracy[1]) if mode == tf.estimator.ModeKeys.EVAL: return tf.estimator.EstimatorSpec( mode, loss=loss, eval_metric_ops=metrics) # Create training op. assert mode == tf.estimator.ModeKeys.TRAIN optimizer = tf.train.AdagradOptimizer(learning_rate=0.1) train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step()) return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op) 可以看见features, labels分别是模型的input,output。所以是必须的。但是在有的模型里面。比如bert预训练的模型里面，我们不需要训练，所以只用到features。]]></content>
      <categories>
        <category>tensorflow</category>
        <category>estimator</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorboard]]></title>
    <url>%2F2019%2Ftensorboard%2F</url>
    <content type="text"><![CDATA[tensorboard比较好的参考资料可参考如下三个链接: https://www.tensorflow.org/guide/summaries_and_tensorboard https://www.tensorflow.org/guide/graph_viz https://www.tensorflow.org/guide/tensorboard_histograms 使用example可以参考:https://github.com/tensorflow/tensorflow/blob/r1.10/tensorflow/examples/tutorials/mnist/mnist_with_summaries.py api可以参考https://www.tensorflow.org/api_docs/python/tf/summary]]></content>
      <categories>
        <category>tensorflow</category>
        <category>tensorboard</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.train.Example]]></title>
    <url>%2F2019%2Ftf-train-Example%2F</url>
    <content type="text"><![CDATA[Example 使用案例tensorflow提供的tf.placeholder这种方式读写不便于分布式。 TensorFlow提供了一个标准的读写格式和存储协议，不仅如此，TensorFlow也提供了基于多线程队列的读取方式，高效而简洁，读取速度也更快。这就是protobuf 比较好的博客参考 tensorflow高级读写教程 比较好的write使用方法如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495class TFRecordsGenerator(object): def __init__(self): self._generator = EncoderGenerator() def set_gmonly_mode(self): self._generator.set_gmonly_mode() def set_allspans_mode(self): self._generator.set_allspans_mode() def is_gmonly_mode(self): return self._generator.is_gmonly_mode() def is_allspans_mode(self): return self._generator.is_allspans_mode() @staticmethod def _to_sequence_example(sample): def _bytes_feature(value): return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value])) # Those two create a simple feature. The first a simple feature with one integer, whereas the second a simple # list of integers as one feature. def _int64_feature(value): """value is a simple integer.""" return tf.train.Feature(int64_list=tf.train.Int64List(value=[value])) def _int64list_feature(value): """value is a list of integers.""" return tf.train.Feature(int64_list=tf.train.Int64List(value=value)) def _int64_feature_list(values): """ values is a list of integers like the words (words = [2,4,6,8,10]) a feature list where each feature has only one number (a list with fixed number of elements, specifically only one)""" return tf.train.FeatureList(feature=[_int64_feature(v) for v in values]) def _int64list_feature_list(values): """ like the chars = [[1,2,3], [4,5], [6], [7,8], [9,10,11,12]] a feature list where each feature can have variable number of ements""" return tf.train.FeatureList(feature=[_int64list_feature(v) for v in values]) def _floatlist_feature_list(values): """ like the chars = [[0.1,0.2,0.3], [0.4,0.5]] a feature list where each feature can have variable number of ements""" def _floatlist_feature(value): """value is a list of integers.""" return tf.train.Feature(float_list=tf.train.FloatList(value=value)) return tf.train.FeatureList(feature=[_floatlist_feature(v) for v in values]) context = tf.train.Features(feature=&#123; "chunk_id": _bytes_feature(sample.chunk_id.encode('utf-8')), "words_len": _int64_feature(sample.words_len), "spans_len": _int64_feature(sample.spans_len), "ground_truth_len": _int64_feature(sample.ground_truth_len) &#125;) feature_list = &#123; "words": _int64_feature_list(sample.words), "chars": _int64list_feature_list(sample.chars), "chars_len": _int64_feature_list(sample.chars_len), "begin_span": _int64_feature_list(sample.begin_spans), "end_span": _int64_feature_list(sample.end_spans), "cand_entities": _int64list_feature_list(sample.cand_entities), "cand_entities_scores": _floatlist_feature_list(sample.cand_entities_scores), "cand_entities_labels": _int64list_feature_list(sample.cand_entities_labels), "cand_entities_len": _int64_feature_list(sample.cand_entities_len), "ground_truth": _int64_feature_list(sample.ground_truth) &#125; if isinstance(sample, SampleEncoded): feature_list["begin_gm"] = _int64_feature_list(sample.begin_gm) feature_list["end_gm"] = _int64_feature_list(sample.end_gm) feature_lists = tf.train.FeatureLists(feature_list=feature_list) sequence_example = tf.train.SequenceExample(context=context, feature_lists=feature_lists) return sequence_example def process(self, filepath): print("processing file: ", filepath) #the name of the dataset. just extract the last part of path filename = os.path.basename(os.path.normpath(filepath))[:-4] # omit the '.txt' output_folder = config.base_folder+"data/tfrecords/"+args.experiment_name+"/" output_folder += "gmonly/" if self.is_gmonly_mode() else "allspans/" if not os.path.exists(output_folder): os.makedirs(output_folder) writer = tf.python_io.TFRecordWriter(output_folder+filename) records_cnt = 0 for sample in self._generator.process(filepath): #print(sample) sequence_example = self._to_sequence_example(sample) # write it to file if sequence_example is not None: writer.write(sequence_example.SerializeToString()) records_cnt += 1 writer.close() print("records_cnt = ", records_cnt) ProtoBuf这是一份很有诚意的 Protocol Buffer 语法详解]]></content>
      <categories>
        <category>tensorflow</category>
        <category>api</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow数据输入]]></title>
    <url>%2F2019%2Ftensorflow%E6%95%B0%E6%8D%AE%E8%BE%93%E5%85%A5%2F</url>
    <content type="text"><![CDATA[简介本篇博客讲解tensorflow如何将数据灌进模型进行训练 tensorflow有两种将数据灌进tensorflow计算图的方法 使用tf.placeholder 使用tf.Data tf.placeholder占位符适用于简单的实验参考: https://www.tensorflow.org/guide/low_level_intro#feeding tf.datatf.data模块是tensorflow将数据流式传输到模型的首选方法。也是本篇博客的重点参考: https://www.tensorflow.org/guide/datasets https://www.tensorflow.org/guide/low_level_intro#datasets 在该模块中TensorFlow引入了两个新的抽象类 tf.data.Dataset表示一系列元素，其中每个元素包含一个或多个 Tensor 对象。例如，在图像管道中，元素可能是单个训练样本，具有一对表示图像数据和标签的张量。 tf.data.Iterator 提供了从数据集中提取元素的主要方法。 tf.data.Datasettf.data.Dataset代表了一系列元素，每个元素包含一个或者多个Tensor对象。例如，在图像管道中，元素可能是单个训练样本，具有一对表示图像数据和标签的张量。可以通过两种不同的方式来创建数据集: 创建来源（例如 Dataset.from_tensor_slices()），以通过一个或多个 tf.Tensor 对象构建数据集。 应用转换（例如 Dataset.batch()），以通过一个或多个 tf.data.Dataset 对象构建数据集。 创建数据集下面将详细讲解如何创建数据集 创建来源tf.data.Dataset提供了四个静态方法来创建数据集 tf.data.Dataset.from_tensors函数签名为def from_tensors(tensors) -&gt; TensorDataset使用方法差不多同下面的tf.data.Dataset.from_tensor_slices tf.data.Dataset.from_tensor_slices函数签名为def from_tensor_slices(tensors) -&gt; TensorSliceDataset代码使用样例如下: 123456789101112131415161718dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))print(dataset1.output_types) # ==&gt; "tf.float32"print(dataset1.output_shapes) # ==&gt; "(10,)"dataset2 = tf.data.Dataset.from_tensor_slices( (tf.random_uniform([4]), tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))print(dataset2.output_types) # ==&gt; "(tf.float32, tf.int32)"print(dataset2.output_shapes) # ==&gt; "((), (100,))"dataset3 = tf.data.Dataset.zip((dataset1, dataset2))print(dataset3.output_types) # ==&gt; (tf.float32, (tf.float32, tf.int32))print(dataset3.output_shapes) # ==&gt; "(10, ((), (100,)))"# 为元素的每个组件命名通常会带来便利性，例如，如果它们表示训练样本的不同特征。dataset4 = tf.data.Dataset.from_tensor_slices(&#123;"a": tf.random_uniform([4]), "b": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)&#125;)print(dataset4.output_types) # ==&gt; "&#123;'a': tf.float32, 'b': tf.int32&#125;"print(dataset4.output_shapes) # ==&gt; "&#123;'a': (), 'b': (100,)&#125;" 和tf.data.Dataset.from_tensors不同在于，这种方法接受tuple和dict作为输入。每一个tensor的第0维的size必须一致。 tf.data.Dataset.from_generator函数签名为def from_generator(generator, output_types, output_shapes=None, args=None) -&gt; Dataset代码样例如下:123456789101112import itertoolsdef gen(): for i in itertools.count(1): yield (i, [1]*i)ds = tf.data.Dataset.from_generator(gen, (tf.int64, tf.int64), (tf.TensorShape([]), tf.TensorShape([None])))value = ds.make_one_shot_iterator().get_next()with tf.Session() as sess: print(sess.run(value)) # (1, array([1])) print(sess.run(value)) # (2, array([1, 1])) 详细使用方法见: https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_generator tf.data.Dataset.range函数签名为def range(*args) -&gt; Dataset使用方法和python内置方法range差不多。可参见文档： https://www.tensorflow.org/api_docs/python/tf/data/Dataset#range 应用转换此部分参考后面的tf.data.Dataset.map(f)部分 tf.data.Iterator tf.data.Iterator提供了从 Dataset中提取元素的主要方法。Iterator.get_next() 返回的操作会在执行时生成 Dataset 的下一个元素，并且此操作通常充当输入管道代码和模型之间的接口。 创建迭代器构建了表示输入数据的 Dataset 后，下一步就是创建 Iterator 来访问该数据集中的元素。tf.data API 目前支持下列迭代器，复杂程度逐渐增大： 单次 可初始化 可重新初始化 可馈送 单次单次迭代器仅仅对数据集进行一次迭代。不需要显示初始化展示代码如下:12345dataset = tf.data.Dataset.range(100)iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next()for i in range(100): value = sess.run(next_element) assert i == value 单词迭代器虽然只可以迭代一次，但是依旧可以在Dataset上对数据集进行操作，使得可以对数据集进行多轮训练。比如我们想在数据集上进行10轮训练，只需要修改上面的代码为:12345dataset = tf.data.Dataset.range(100).repeat(10)iterator = dataset.make_one_shot_iterator()next_element = iterator.get_next()for i in range(100): value = sess.run(next_element) assert i == value 可初始化您需要先运行显式 iterator.initializer 操作，然后才能使用可初始化迭代器。相比单次迭代器，其每初始化一次就可以迭代一次数据。故其相对于单次迭代器如果需要对数据进行多轮训练只能在Dataset上面做手脚不同，可初始化迭代器可以对迭代器进行多次初始化从而进行多次迭代。下面是该迭代器使用方式: 12345678910111213141516max_value = tf.placeholder(tf.int64, shape=[])dataset = tf.data.Dataset.range(max_value)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# Initialize an iterator over a dataset with 10 elements.sess.run(iterator.initializer, feed_dict=&#123;max_value: 10&#125;)for i in range(10): value = sess.run(next_element) assert i == value# Initialize the same iterator over a dataset with 100 elements.sess.run(iterator.initializer, feed_dict=&#123;max_value: 100&#125;)for i in range(100): value = sess.run(next_element) assert i == value 可重新初始化可重新初始化迭代器可以通过多个不同的 Dataset 对象进行初始化。故而其相对于可初始化迭代器，其可以使用一个Dataset进行训练，另外一个Dataset进行验证使用代码如下: 123456789101112131415161718192021222324252627"""可重新初始化迭代器使用方法"""import tensorflow as tf# 创造数据集training_dataset = tf.data.Dataset.range(6)validation_dataset = tf.data.Dataset.range(5)# 创造课重新初始化迭代器iterator: tf.data.Iterator = tf.data.Iterator.from_structure(training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()# 创建init_optraining_init_op = iterator.make_initializer(training_dataset)validation_init_op = iterator.make_initializer(validation_dataset)sess = tf.InteractiveSession()for i in range(3): sess.run(training_init_op) for j in range(5): print('train', j, sess.run(next_element), sep='\t') sess.run(validation_init_op) for k in range(4): print('test', k, sess.run(next_element), sep='\t') 运行上面的代码其结果为123456789101112131415161718192021222324252627train 0 0train 1 1train 2 2train 3 3train 4 4test 0 0test 1 1test 2 2test 3 3train 0 0train 1 1train 2 2train 3 3train 4 4test 0 0test 1 1test 2 2test 3 3train 0 0train 1 1train 2 2train 3 3train 4 4test 0 0test 1 1test 2 2test 3 3 可以看到上面可重新初始化迭代器可以在多个数据集上切换，即可以做early_stop这样的防止过拟合的手段。 但是其在每一次切换数据的时候，需要进行初始化，然后导致即使之前的数据集没跑完 (比如上面每次在training_dataset上跑，因为每次train的循环取5个数据，但是整个数据集6个数据，还有个数据没用上)，也没有机会使用后面的数据了。 通过合适的手段可以使每一次初始化之前，数据跑满，即可实现每次跑完一批数据，然后进行验证，从而实现keras.fit的early_stop功能。代码如下 123456789101112131415for i in range(3): sess.run(training_init_op) while True: try: print('train', sess.run(next_element), sep='\t') except tf.errors.OutOfRangeError: break sess.run(validation_init_op) while True: try: print('test', sess.run(next_element), sep='\t') except tf.errors.OutOfRangeError: break 可馈送可馈送迭代器可以实现可重新初始化迭代器的所有功能，而且相较后者迭代器之间进行切换时需要从数据集的开头初始化迭代器不同，可馈送迭代器可以从数据集没跑完的那个断点处接着跑。 这样的一个应用场景是，比如说我们在做机器翻译的时候，如果数据集非常大，那么我们可能只需要在数据集上跑五六个epoch即可得到一个很好的模型，这个时候使用可重新初始化迭代器(或者等同于keras)，我们很难去做early_stop。原因是如果我们要充分利用所有数据，我们可重新初始化迭代器的验证点只能在所有训练数据跑完一轮才可以，但是我们的模型就跑不了几轮，所以很难去做early_stop。如果我们想做early_stop，那么我们就会损失训练数据。而可馈送迭代器就是解决这个问题的，其验证点可以顺便放在训练数据的哪个点都可以。 如下是可馈送迭代器的使用代码示例: 123456789101112131415161718192021222324252627282930"""可馈送迭代器使用方法"""import tensorflow as tf# 创造数据集training_dataset = tf.data.Dataset.range(6).repeat()validation_dataset = tf.data.Dataset.range(5)# 创造可馈送迭代器handle = tf.placeholder(tf.string, shape=[])iterator = tf.data.Iterator.from_string_handle(handle, training_dataset.output_types, training_dataset.output_shapes)next_element = iterator.get_next()# 创造数据集的迭代器，一般的可以为单次迭代器和可初始化迭代器training_iterator = training_dataset.make_one_shot_iterator()validation_iterator = validation_dataset.make_initializable_iterator()sess = tf.InteractiveSession()# 给每个数据集的迭代器创造一个tensor, 该tensor作为可馈送迭代器的handle的输入training_handle = sess.run(training_iterator.string_handle())validation_handle = sess.run(validation_iterator.string_handle())for i in range(3): for j in range(5): print('train', j, sess.run(next_element, feed_dict=&#123;handle: training_handle&#125;), sep='\t') sess.run(validation_iterator.initializer) for k in range(4): print('test', k, sess.run(next_element, feed_dict=&#123;handle: validation_handle&#125;), sep='\t') 上面代码的输出结果为: 123456789101112131415161718192021222324252627train 0 0train 1 1train 2 2train 3 3train 4 4test 0 0test 1 1test 2 2test 3 3train 0 5 ### 注意这一行，其接着上次train的断点处接着load数据train 1 0train 2 1train 3 2train 4 3test 0 0test 1 1test 2 2test 3 3train 0 4train 1 5train 2 0train 3 1train 4 2test 0 0test 1 1test 2 2test 3 3 保存迭代器状态有时候我们不能从一开始就可以训练好模型(比如资源比较紧迫的实验室)。这个时候就需要将迭代器的状态也给存储下来，以便从数据的某个点继续训练。这部分可以详细看https://www.tensorflow.org/guide/datasets#saving_iterator_state 读取数据tf.data可以从四种格式里面读取数据 Numpy数组 TFRecord数据 文本数据 CSV数据详情可以看 https://www.tensorflow.org/guide/datasets#reading_input_data tf.data.Dataset.map()预处理数据tf.data.Dataset.map(f)转换通过将指定函数 f 应用于输入数据集的每个元素来生成新数据集。同时因为tensorflow使用静态图的缘故，很难使用外部python库去处理一些逻辑，但是在解析数据的时候，调用外部python库很有用，tensorflow通过提供tf.py_func()来将外部库的逻辑纳入tensorflow处理数据里面这块可以参考: https://www.tensorflow.org/guide/datasets#preprocessing_data_with_datasetmap 批处理处理数据tf.data.Dataset提供了两个方法来进行batch训练: Dataset.batch，简单的批处理。适用于每条数据的张量形状一样 Dataset.padded_batch，适用于自然语言处理的seq类型的每个样本数据长度不一的数据。 详细的两个方法的使用可以参考: https://www.tensorflow.org/guide/datasets#batching_dataset_elements 训练工作流程资料 https://www.tensorflow.org/guide/datasets#training_workflows 简要的讲解了tf.data如何将数据灌入模型 api这部分讲解一些Dataset类的方法的简介说明 zip(datasets)静态方法。类似于python内置函数zip。转换多个数据集为一个多输入数据集 concatenate(self, dataset)拼接两个dataset为一个更大的数据集 prefetch(self, buffer_size)list_files(file_pattern, shuffle=None, seed=None)静态方法。列出有哪些文件 repeat(self, count=None)重复数据集一定次数。count也叫epoch shuffle(self, buffer_size, seed=None, reshuffle_each_iteration=None)cache(self, filename=””)cache数据集 take(self, count)取出数据集里面的前count个数据。如果count是-1或者它大于数据集本身的大小就取出所有数据使用方法如下: 1234567891011121314import tensorflow as tfdataset = tf.data.Dataset.range(10).take(4)iterator = dataset.make_one_shot_iterator()next_ele = iterator.get_next()with tf.Session() as sess: while 1: try: print(sess.run(next_ele)) except tf.errors.OutOfRangeError: print('iterator over') break 输出为: 123450123iterator over skip(self, count)将前count个数据舍弃。1234567891011121314import tensorflow as tfdataset = tf.data.Dataset.range(10).take(4).skip(1)iterator = dataset.make_one_shot_iterator()next_ele = iterator.get_next()with tf.Session() as sess: while 1: try: print(sess.run(next_ele)) except tf.errors.OutOfRangeError: print('iterator over') break 输出为: 1234123iterator over shard(self, num_shards, index)这个方法在分布式训练里面特别有用。作用是将数据分成num_shards份分布在不同机器上训练。详细使用可以看官方文档 https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shard flat_map(self, map_func)将数据集里面的每个元素展平然后拼接。 12345# NOTE: The following examples use `&#123; ... &#125;` to represent the# contents of a dataset. '[...]' represents a tensor.a = &#123;[1,2,3,4,5], [6,7,8,9], [10]&#125;a.flat_map(lambda x: Dataset.from_tensor_slices(x)) == &#123;[1,2,3,4,5,6,7,8,9,10]&#125; filter(self, predicate)可以参考 机器翻译数据集用filter过滤掉src或tgt数据里面长度为0的坏数据 apply(self, transformation_func)https://www.tensorflow.org/api_docs/python/tf/data/Dataset#apply window(self, size, shift=None, stride=1, drop_remainder=False)tf.data.Dataset.range(7).window(2) 产生 [{0, 1}, {2, 3}, {4, 5}, {6}]这样的数据。详情看官方文档 reduce(self, initial_state, reduce_func)该函数返回一个tensor，而不是一个Dataset类使用如下:12345import tensorflow as tfimport numpy as npt = tf.data.Dataset.range(5).reduce(np.int64(0), lambda x, y: x + y)with tf.Session() as sess: print(sess.run(t)) # 10 with_options(self, options)]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow debug]]></title>
    <url>%2F2019%2Ftensorflow-debug%2F</url>
    <content type="text"><![CDATA[tfdbgtfdbg的目的由于 TensorFlow 的计算图模式，使用通用调试程序（如 Python 的 pdb）很难完成调试。 虽然这种静态图很难调试，但是因为静态计算的机制相比动态计算允许编译器进行更大程度的优化，更符合工业要求(空间、时间上考虑)。 本篇博客记录如何使用Tensorflow内置工具tfdbg进行debug。 tfdbg的介绍tfdbg有两种debug方式进行调试程序。 基于命令行调试。类为tf_debug.LocalCLIDebugWrapperSession 图形界面调试，需要借助于Tensorboard。类为tf_debug.TensorBoardDebugWrapperSession 详细的debug指南详见tensorflow debug官方文档。 使用Tensorboard进行图形界面调试详见The Debugger Dashboard 张量过滤器编写使用tfdbg进行调试的时候，需要对错误进行调试。比如在训练过程中经常出现nan和inf的问题。对于这种错误我们需要进行调试。这个时候我们就需要过滤出出现inf和nan的tensor。因此调试tfdbg的代码工作就是编写这些张量过滤器 张量过滤器的签名是def function(datum, tensor) -&gt; bool 如下是tensorflow内置的has_inf_or_nan张量过滤器，这个过滤器用来判断tensor是否包含inf和nan其源码为123456789101112131415161718192021222324252627282930def has_inf_or_nan(datum, tensor): """A predicate for whether a tensor consists of any bad numerical values. This predicate is common enough to merit definition in this module. Bad numerical values include `nan`s and `inf`s. The signature of this function follows the requirement of the method `DebugDumpDir.find()`. Args: datum: (`DebugTensorDatum`) Datum metadata. tensor: (`numpy.ndarray` or None) Value of the tensor. None represents an uninitialized tensor. Returns: (`bool`) True if and only if tensor consists of any nan or inf values. """ _ = datum # Datum metadata is unused in this predicate. if isinstance(tensor, InconvertibleTensorProto): # Uninitialized tensor doesn't have bad numerical values. # Also return False for data types that cannot be represented as numpy # arrays. return False elif (np.issubdtype(tensor.dtype, np.floating) or np.issubdtype(tensor.dtype, np.complex) or np.issubdtype(tensor.dtype, np.integer)): return np.any(np.isnan(tensor)) or np.any(np.isinf(tensor)) else: return False 编写完之后需要使用sess.add_tensor_filter注册张量过滤器才可以使用 使用tfdbg调试keras只需要修改非常少的代码，具体介绍及example可见 https://www.tensorflow.org/guide/debugger#debugging_keras_models_with_tfdbg https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/debug/examples/debug_keras.py tf.debuggingtodo: 需要研究一下怎么使用 输出张量这个类似于一般的python程序的print函数，tensorflow实现了tf.print这个函数用于打印张量。使用方式可见tensorflow输出张量。个人觉得这个东西有点鸡肋，在使用上比较繁琐]]></content>
      <categories>
        <category>tensorflow</category>
        <category>debug</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>keras</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tf.global_variables_initializer]]></title>
    <url>%2F2019%2Ftf-global-variables-initializer%2F</url>
    <content type="text"><![CDATA[该篇博客主要记录一些tensorflow api理解 tf.global_variables_initializer表面上通常我们可以在一些tensorflow程序中看到有类似这样的一句代码sess.run(tf.global_variables_initializer())那么为什么会有这么一句呢。那是因为变量必须先初始化之后才可以使用。这也意味着如果你的程序没有变量的话就可以不用写这句代码。如下代码: 1234567import tensorflow as tftensor = tf.range(10)out = tf.add(tensor, tensor)with tf.Session() as sess: print(out.eval()) 上面代码将会打印如下输出:1[ 0 2 4 6 8 10 12 14 16 18] 因为上面的代码没有变量(准确说只要没有global_variables就行)，所以我们不用写这一句。 变量类型默认情况下，每个变量tf.Variable放置在两个集合中。 tf.GraphKeys.GLOBAL_VARIABLES - 可以在多台设备间共享的变量， tf.GraphKeys.TRAINABLE_VARIABLES - TensorFlow 将计算其梯度的变量。 还有如果你希望你的变量不被训练，可以添加到 tf.GraphKeys.LOCAL_VARIABLES。例如，以下代码段展示了如何将名为 my_local 的变量添加到此集合中： 1my_local = tf.get_variable("my_local", shape=(),collections=[tf.GraphKeys.LOCAL_VARIABLES]) 或者，您可以指定 trainable=False（作为 tf.get_variable 的参数）： 1my_non_trainable = tf.get_variable("my_non_trainable", shape=(), trainable=False) 实际上由上面其实我们就可以看出，tf.global_variables_initializer()实际上是在初始化tf.GraphKeys.GLOBAL_VARIABLES集合中的所有变量. 注意点默认情况下，tf.global_variables_initializer 不会指定变量的初始化顺序。因此，如果变量的初始值取决于另一变量的值，那么很有可能会出现错误。任何时候，如果您在并非所有变量都已初始化的上下文中使用某个变量值（例如在初始化某个变量时使用另一变量的值），最好使用 variable.initialized_value()，而非 variable：12v = tf.get_variable(&quot;v&quot;, shape=(), initializer=tf.zeros_initializer())w = tf.get_variable(&quot;w&quot;, initializer=v.initialized_value() + 1) 参考 https://www.tensorflow.org/guide/variables#initializing_variables tf.control_dependencies]]></content>
      <categories>
        <category>tensorflow</category>
        <category>api</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow low level api]]></title>
    <url>%2F2019%2Ftensorflow-low-level-api%2F</url>
    <content type="text"><![CDATA[低级别的API构建TensorFlow模型使用低级别的Tensorflow API (Tensorflow Core)构建tensorflow程序Tensorflow Core 程序可以看成互相独立的两部分: 构建计算图（tf.Graph） 运行计算图（tf.Session） 示例下面的代码code11234567891011# 构建计算图a = tf.constant(3.0, dtype=tf.float32)b = tf.constant(4.0)total = a + bprint(a)print(b)print(total)# 运行计算图sess = tf.Session()print(sess.run(total)) 打印语句会生成1234Tensor(&quot;Const:0&quot;, shape=(), dtype=float32)Tensor(&quot;Const_1:0&quot;, shape=(), dtype=float32)Tensor(&quot;add:0&quot;, shape=(), dtype=float32)7.0 tf.Graph计算图是排列成一个图的一系列 TensorFlow 指令。图由两种类型的对象组成。 操作（简称“op”）：图的节点。操作描述了消耗和生成张量的计算。 张量：图的边。它们代表将流经图的值。大多数 TensorFlow 函数会返回 tf.Tensors。 计算图是有向无环图 操作(tf.Operation)图的节点(tf.Operation)上面的代码code1使用tensorboard可视化为里面total = a + b这个a+b语句会产生一个Add operation，即图的节点，该节点接受两个边(Tensor)a和b然后产生total这个边(其实调用tf.constant()也会产生一个新的tf.Operation, 并返回一个Tensor, 这个Tensor被赋给了a) 张量(tf.Tensor)图的边(tf.Tensor) 普通的张量 tf.Tensor 特殊的张量 tf.Variable tf.constant tf.placeholder tf.SparseTensor 普通的张量普通的张量类似于code1的里面的total详细介绍可参考: https://www.tensorflow.org/guide/tensors tf.Variable与 tf.Tensor 对象不同，tf.Variable 存在于单个 session.run 调用的上下文之外。 在 TensorFlow 内部，tf.Variable 会存储持久性张量。具体 op 允许您读取和修改此张量的值。这些修改在多个 tf.Session 之间是可见的，因此对于一个 tf.Variable，多个工作器可以看到相同的值。 我们构建tensorflow模型并进行训练的目的便是学习到一组Variable值，这些Variable代表了一个学习后的模型。 虽然tf.Variable是一种特殊的张量(特殊点在于其会被持久化，以及修改在多个tf.Session之间是可见的)。但是其还是张量，因此在tensorflow中使用tf.Variable的时候，只需将其视为普通的tf.Tensor就行 具体变量如何重用共享可参考: https://www.tensorflow.org/guide/variables 会话对于构建的计算图，我们需要使用tf.Session封装TensorFlow 运行时的状态，并运行 TensorFlow 操作。如上面的code1。如果说 tf.Graph 像一个 .py 文件，那么 tf.Session 就像一个 python 可执行对象。绝大部分tensorflow程序会话是使用tf.Session来实现的。特殊的会话还有很多。下面列出的这些会话主要用于进行debug作用，还是比较常用的一种 tf.InteractiveSession tf_debug.LocalCLIDebugWrapperSession tf_debug.TensorBoardDebugWrapperSession 下图展示四种Session类的继承关系 tf.Session这是最常用的会话 tf.InteractiveSessiontf.InteractiveSession()是一种交互式的session方式，它让自己成为了默认的session，也就是说用户在不需要指明用哪个session运行的情况下，就可以运行起来，这就是默认的好处。这样的话就是run()和eval()函数可以不指明session啦。 对比一下：12345678import tensorflow as tf import numpy as npa=tf.constant([[1., 2., 3.],[4., 5., 6.]])b=np.float32(np.random.randn(3,2))c=tf.matmul(a,b)init=tf.global_variables_initializer()sess=tf.Session() print(c.eval()) 上面的代码编译是错误的，显示错误如下：ValueError: Cannot evaluate tensor using eval(): No default session is registered. Use with sess.as_default() or pass an explicit session to eval(session=sess) 12345678import tensorflow as tf import numpy as npa=tf.constant([[1., 2., 3.],[4., 5., 6.]])b=np.float32(np.random.randn(3,2))c=tf.matmul(a,b)init=tf.global_variables_initializer()sess=tf.InteractiveSession() print(c.eval()) 而用InteractiveSession()就不会出错，说白了InteractiveSession()相当于：12sess=tf.Session()with sess.as_default(): 换句话说，如果说想让sess=tf.Session()起到作用，一种方法是上面的with sess.as_default()；另外一种方法是12sess=tf.Session() print(c.eval(session=sess)) 其实还有一种方法也是with，如下：12345678import tensorflow as tf import numpy as npa=tf.constant([[1., 2., 3.],[4., 5., 6.]])b=np.float32(np.random.randn(3,2))c=tf.matmul(a,b)init=tf.global_variables_initializer()with tf.Session() as sess: #print (sess.run(c)) print(c.eval()) 总结：tf.InteractiveSession()默认自己就是用户要操作的session，而tf.Session()没有这个默认，因此用eval()启动计算时需要指明session。 tf_debug.LocalCLIDebugWrapperSession详细参考: https://www.tensorflow.org/guide/debugger#debugging_model_training_with_tfdbg tf_debug.TensorBoardDebugWrapperSession详细参考: https://github.com/tensorflow/tensorboard/blob/master/tensorboard/plugins/debugger/README.md 图和会话详细的图和会话交互参考: https://www.tensorflow.org/guide/graphs]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow bug Saver保存模型版本格式之间不一致问题]]></title>
    <url>%2F2019%2Ftensorflow-bug-Saver%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%E7%89%88%E6%9C%AC%E6%A0%BC%E5%BC%8F%E4%B9%8B%E9%97%B4%E4%B8%8D%E4%B8%80%E8%87%B4%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[该篇博客记录使用tf.train.Saver在使用不同版本的protobuf来进行持久化模型遇见的不一致问题复现代码如下所示:123456789101112131415161718import tensorflow as tffrom tensorflow.core.protobuf import saver_pb2# Create some variables. v1 = tf.get_variable("v1", shape=[3], initializer = tf.zeros_initializer)v2 = tf.get_variable("v2", shape=[5], initializer = tf.zeros_initializer)inc_v1 = v1.assign(v1+1)dec_v2 = v2.assign(v2-1)init_op = tf.global_variables_initializer()saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V1)# saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V2)with tf.Session() as sess: sess.run(init_op) inc_v1.op.run() dec_v2.op.run() save_path = saver.save(sess, "tf_format1/model.ckpt") print("Model saved in path: %s" % save_path) 当write_version为V1的时候报错，原因是V1版本的不会自己建立文件夹]]></content>
      <categories>
        <category>tensorflow</category>
        <category>bug</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
        <tag>bug</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow保存模型与恢复]]></title>
    <url>%2F2019%2Ftensorflow%E4%BF%9D%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[tensorflow 模型保存tensorflow模型保存有两种方式。一种tensorflow本身的保存方式，使用tf.train.Saver进行保存，即tf格式一种是当使用tf.Keras.Model作为模型的时候还可以使用h5方式保存，即h5格式 tf格式tf模式有两种保存文件的方式。下面是如何使用tf格式保存模型123456789101112131415161718import tensorflow as tffrom tensorflow.core.protobuf import saver_pb2# Create some variables. v1 = tf.get_variable("v1", shape=[3], initializer = tf.zeros_initializer)v2 = tf.get_variable("v2", shape=[5], initializer = tf.zeros_initializer)inc_v1 = v1.assign(v1+1)dec_v2 = v2.assign(v2-1)init_op = tf.global_variables_initializer()saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V1)# saver = tf.train.Saver(write_version=saver_pb2.SaverDef.V2)with tf.Session() as sess: sess.run(init_op) inc_v1.op.run() dec_v2.op.run() save_path = saver.save(sess, "tf_format1/model.ckpt") print("Model saved in path: %s" % save_path) 如上writer_version分别可以为V1或者V2V1版本是要被tensorflow抛弃的一个版本。默认和推荐的版本都是V2V2会产生4个文件 checkpoint，记录最新模型的checkpoint model.ckpt.data-00000-of-00001 , 网络权重信息 model.ckpt.index # 存储Variables的索引 model.ckpt.meta 一个协议缓冲，保存tensorflow中完整的graph、variables、operation、collection。当saver.save传入write_meta_graph=False时，将不会产生这个文件。keras模型保存时当使用tf格式存储时，这个参数设置的就是false。 具体使用方式可参考 https://www.tensorflow.org/guide/saved_model h5格式该种格式在使用Keras模型的时候才可以使用，使用model.save_weights方法然后传参save_format=h5, 后面使用的是hdf5来存储训练的模型。Keras还可以使用tf格式保存，其本质就是调用tf.train.Saver。 tensorflow 模型恢复具体可参考 https://www.tensorflow.org/guide/saved_model#restore_variables]]></content>
      <categories>
        <category>tensorflow</category>
      </categories>
      <tags>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git总结]]></title>
    <url>%2F2019%2Fgit%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[fork的仓库合并代码merge的三种方式在git中分支merge通常有三种方式(在github上提交pr给原作者, 合并的方式也是这三种) 普通的merge这是入门级别的merge方式。基本只要了解这种方式就会知道该命令。这种方式会保留所有的信息, 在稍微大点的项目中就不行了。因为他会把所有的commit都保留下来,比如说两个分支的merge信息也会保留下来。下图就是这种merge带来的繁琐graph squash merge (推荐)squash merge 是github上常用的merge方式。通常一个pr会有多个commit, 有些commit是对上一个commit的调整(比如说上一个commit的代码不符合pep8, 然后这个commit对上一个commit进行modify)。我们按照普通的merge进行merge的话,回报这些繁琐的信息保留下来, 这是不好的。这个时候我们需要将这个pr的多个commit合并为一个commit, 并选择一些信息作为最终commit的信息。这个时候就需要squash merge了 rebase merge (推荐)普通merge会保留许多合并点, 但是我们并不想看到这些点, 而是只想看到一条线的提交记录。比如下图这样的 这个时候我们可以使用rebase merge。他会使用rebase将commit合并为一条线, 这样可以保证相对来说比较简洁 比较好的博客merge的三种合并方式git rebase详解git rebasesquash merge 删除远程分支后, branch -a 还是能看到的解决办法问题的重现:假设现在有个仓库owner/one_project, 然后我clone一个到我的github上my/one_project。现在我在本地新建一个分支dev_fixbug并修改代码、add、commit、push到my/one_project。然后我们建立一个pr将dev_fixbug提交给owner/one_project。 owner在rebase merge了我们的代码后, 我们可以将这个dev_fixbug在github上直接delete掉, 或者本地执行git push origin --delete dev_fixbug。这个分支在远程remote上确实被删掉了。但是我们本地依然还会保留着remotes/origin/dev_fixbug(使用git branch -a即可查看)解决办法是:12$ git remote show origin // 查看远程分支, 既可以查看到删除分支的情况$ git remote prune origin // 删除了本地的remotes/origin/dev_fixbug 这个时候我们使用git branch -a会发现remotes/origin/dev_fixbug被删除掉了 git fetch vs git pull两者都是用来从remote上拉取代码的git pull 可以粗略的理解为git fetch &amp;&amp; git merge讲解区别的比较好的博客详解git fetch与git pull的区别 syncing a fork保持fork的项目同原始项目同步github官方文档(其实在github上的最简单的姿势是取upstream所在的仓库, 然后点击fork,github就会将你的仓库同upstream仓库同步) pull request代码到别的仓库标准姿势如下 假设该仓库为owner/repo.git, 首先要做的是fork该仓库到我们的账户下, 假设fork之后是my/repo.git 将我们的仓库clone下来 新建一个分支, 写代码, add, commit, push。 在github上将pr提交给owner, 待owner将分支合并到master即可将新建的分支在远程删掉 上面的姿势的好处有如下几点: 我们不用将owner所拥有的远程仓库添加到我们本地代码作为upstream, 同步代码我们只需要在owner所在仓库的github主页上点击fork即可。 因为我们使用的分支是新建的分支, 我们在github上fork代码去做同步, 然后pull下来是不影响新建的这个分支的, 同时我们可以使用merge将主分支的修改合并到新建的分支上保持同步。 因为我们使用的是新建的分支, 这个分支被owner所接受后。我们在owner点击fork同步代码不会被该新建的分支所干扰(如果我们使用的是master分支做pull request的话, 我们提交后, 如果做同步代码的话会发生upstream的master合并到origin的master上面, 因为我们的master提交记录和owner上的提交记录不一致了。使用新建的分支做pr, 这样会使得我们的master分支始终和owner的master分支的log一致) git 服务搭建系统gogs]]></content>
      <categories>
        <category>tools</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
</search>
